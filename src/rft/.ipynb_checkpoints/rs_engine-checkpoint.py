"""
è¿™æ˜¯rftçš„ç¬¬ä¸€æ­¥
æ‹’ç»é‡‡æ ·ï¼Œç”¨SFT_V3å»è·‘æ•°æ®é›†ï¼ˆå‡ºäºæ—¶é—´å’Œè´¹ç”¨ï¼Œåªè·‘äº†å‰10kæ•°æ®ï¼‰
å¯ä»¥å¾—åˆ°æ¯ä¸ªinstructionå¯¹åº”æ¨¡å‹çš„10ä¸ª
"""

import json
import os
import math
import time
from vllm import LLM, SamplingParams

# --- è·¯å¾„é…ç½® ---
MODEL_PATH = "models/sft_v3_merged_model"
INPUT_PATH = "data/processed/magicoder_final_cleaned.jsonl"
# åŠ ä¸Š v2 æ ‡è¯†ï¼ŒåŒºåˆ†ä½ ä¹‹å‰å¤±è´¥çš„é‚£æ¬¡
OUTPUT_PATH = "data/rft/v3/v3_rs_candidates.jsonl" 

# --- ç¡¬ä»¶ä¸æ€§èƒ½é…ç½® (é’ˆå¯¹ 4090D 24G ä¼˜åŒ–) ---
GPU_UTIL = 0.85          # é¢„ç•™æ˜¾å­˜ï¼Œé˜²æ­¢è°ƒåº¦å³°å€¼ OOM
MAX_MODEL_LEN = 2048     # ç¼©çŸ­ä¸Šä¸‹æ–‡ä»¥å®¹çº³æ›´å¤šå¹¶å‘
MAX_NUM_SEQS = 128       # é€‚ä¸­çš„å¹¶å‘æ•°ï¼Œå¹³è¡¡ååé‡ä¸å†…å­˜ Swap å‹åŠ›
SWAP_SPACE = 16          # 16GB CPU äº¤æ¢ç©ºé—´
ENFORCE_EAGER = True     # ç¦ç”¨ CUDA Graphï¼Œé‡Šæ”¾ ~2G æ˜¾å­˜

# --- é‡‡æ ·ç­–ç•¥é…ç½® ---
DATA_LIMIT = 10000       # æœ¬æ¬¡å®éªŒå…ˆè·‘ 10k
N_CANDIDATES = 10        # å¢åŠ é‡‡æ ·æ·±åº¦ï¼Œç›®æ ‡æ˜¯æŠŠç­›é€‰ç‡ä» 1% æå‡åˆ° 5% ä»¥ä¸Š
CHUNK_SIZE = 1000        # æ¯ 1000 æ¡æŒ‡ä»¤ä½œä¸ºä¸€ä¸ª Chunk æäº¤ï¼Œé™ä½å†…å­˜å‹åŠ›

def run_sampling():
    start_time = time.time()
    
    # 1. åˆå§‹åŒ– vLLM å¼•æ“
    print(f"ğŸš€ æ­£åœ¨åˆå§‹åŒ–å¼•æ“: {MODEL_PATH}")
    
    # ç”¨vLLMå¼•æ“åŠ è½½æ¨¡å‹
    llm = LLM(
        model=MODEL_PATH,
        trust_remote_code=True,
        gpu_memory_utilization=GPU_UTIL,
        max_model_len=MAX_MODEL_LEN,
        max_num_seqs=MAX_NUM_SEQS,
        enforce_eager=ENFORCE_EAGER,  # ç¦ç”¨cuda graph ç›´æ¥çœä¸‹çº¦2Gæ˜¾å­˜
        tensor_parallel_size=1,  # å•å¡è¿è¡Œ ä¸å¼€å¯å¼ é‡å¹¶è¡Œ
        swap_space=SWAP_SPACE  # æ˜¾å­˜ç´§å¼ æ—¶åˆ©ç”¨å†…å­˜ä¿è¯ç¨‹åºä¸å¥”æºƒ
    )
    
    # 2. è®¾ç½®é‡‡æ ·å‚æ•° (Best-of-N æ¨¡å¼)
    sampling_params = SamplingParams(
        n=N_CANDIDATES,      # å…³é”®ï¼šå¯¹æ¯ä¸ª prompt ç”Ÿæˆ 10 ä¸ªå›ç­”  vllmèƒ½ç›´æ¥æŒ‡å®šç”Ÿæˆå¤šå°‘å›ç­”ï¼Œä½†ä¸ç†è§£ä¹‹åå¤šä¸ªå›ç­”çš„æ•°æ˜¯å¦‚ä½•åŒºåˆ†çš„
        temperature=0.8,     # è¾ƒé«˜æ¸©åº¦ï¼šä¸ºäº†è®©æ¨¡å‹â€œå‘æ•£â€ï¼Œç”Ÿæˆä¸åŒçš„é€»è¾‘ï¼Œæ–¹ä¾¿æŒ‘å¥½çš„   æ¸©åº¦å†³å®šæ¨¡å‹ç”Ÿæˆå†…å®¹çš„å‘æ•£æ€§
        top_p=0.95,          # æˆªæ–­æ¦‚ç‡ï¼šä¿è¯ç”Ÿæˆè´¨é‡ï¼Œä¸è‡³äºä¹±è¯´è¯
        max_tokens=1024,     # é™åˆ¶æ¯ä¸ªå›ç­”çš„æœ€é•¿ token æ•°
        stop=["<|im_end|>", "<|endoftext|>"] # åœæ­¢ç¬¦ï¼Œé˜²æ­¢æ¨¡å‹å¤è¯»æˆ–èƒ¡è¨€ä¹±è¯­   è¿™ä¸ªåœæ­¢ç¬¦å’Œæ¨¡å‹è®­ç»ƒå­¦åˆ°çš„è‡ªå·±ç”Ÿæˆçš„ä¼šä¸ä¼šæœ‰å†²çª? GPTç»™çš„è§£é‡Šæ˜¯ä¸ä¼šå†²çªæ˜¯ä¸€ä¸ªåŒä¿é™©ä½†è¿˜æ˜¯ä¸å¤ªç†è§£?
    )

    # 3. åŠ è½½åŸå§‹æ•°æ®å¹¶æ‰“ä¸Šæ ‡è®°
    all_data = []
    if not os.path.exists(INPUT_PATH):
        print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {INPUT_PATH}")
        return

    with open(INPUT_PATH, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            if i >= DATA_LIMIT: break
            item = json.loads(line)
            # æ„é€  ChatML æç¤ºè¯
            prompt = f"<|im_start|>user\n{item['instruction']}\n<|im_end|>\n<|im_start|>assistant\n"
            # è®°å½•åŸå§‹ç´¢å¼•ï¼Œæ–¹ä¾¿åç»­åˆå¹¶æ•°æ®æ—¶å»é‡  å°±æ˜¯ä»¥åå¯èƒ½åŠ å¤§æ•°æ®é‡è®­ç»ƒ è¿˜æœ‰æ˜¯æ–¹ä¾¿å›æº¯æ‰¾æŒ‡ä»¤
            # idx_maeker is mainly used for solving Deduplication, merging, and traceability issues
            item['idx_marker'] = f"part1_{i}"  
            item['formatted_prompt'] = prompt
            all_data.append(item)

    total_items = len(all_data)
    # ceilingæ˜¯å¤©èŠ±æ¿ æ‰€ä»¥ceilæ˜¯å‘ä¸Šå–æ•´
    num_chunks = math.ceil(total_items / CHUNK_SIZE)
    print(f"ğŸ“¦ å·²åŠ è½½ {total_items} æ¡æŒ‡ä»¤ã€‚åˆ† {num_chunks} ä¸ªæ‰¹æ¬¡æ‰§è¡Œã€‚")

    # 4. å¾ªç¯åˆ†æ®µå¤„ç†
    os.makedirs(os.path.dirname(OUTPUT_PATH), exist_ok=True)
    
    # ä½¿ç”¨è¿½åŠ æ¨¡å¼ 'a'ï¼Œå³ä½¿è„šæœ¬ä¸­é€”å´©æºƒï¼Œå·²ç”Ÿæˆçš„ä¹Ÿä¸ä¸¢
    with open(OUTPUT_PATH, 'a', encoding='utf-8') as f_out:
        for chunk_idx in range(num_chunks):
            # è·å–æ¯ä¸ªæ‰¹æ¬¡çš„æ•°æ®èŒƒå›´
            start = chunk_idx * CHUNK_SIZE
            end = min(start + CHUNK_SIZE, total_items)
            
            chunk_data = all_data[start:end]
            chunk_prompts = [d['formatted_prompt'] for d in chunk_data]
            
            print(f"\nâš¡ æ‰¹æ¬¡ {chunk_idx + 1}/{num_chunks} | è¿›åº¦: {start}-{end}")
            
            # æ‰§è¡Œæ¨ç†
            outputs = llm.generate(chunk_prompts, sampling_params, use_tqdm=True)

            # ä¿å­˜ç»“æœ
            for j, output in enumerate(outputs):
                result = {
                    "idx_marker": chunk_data[j]['idx_marker'],
                    "instruction": chunk_data[j]['instruction'],
                    "responses": [o.text.strip() for o in output.outputs]
                }
                f_out.write(json.dumps(result, ensure_ascii=False) + "\n")
            
            f_out.flush() # æ‰¹æ¬¡å®Œæˆå³åˆ»è½ç›˜

    end_time = time.time()
    duration = (end_time - start_time) / 60
    print(f"\nâœ¨ é‡‡æ ·å®Œæˆï¼è€—æ—¶: {duration:.2f} åˆ†é’Ÿ")
    print(f"ğŸ“‚ æ•°æ®ä¿å­˜åœ¨: {OUTPUT_PATH}")

if __name__ == "__main__":
    run_sampling()