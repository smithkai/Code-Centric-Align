import torch
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

base_model_path = "models/Qwen2.5-Coder-7B"
adapter_path = "models/sft_output_v3"
save_path = "models/sft_v3_merged_model"

# 1. ä¼˜å…ˆä» adapter_path åŠ è½½ Tokenizerï¼Œå› ä¸ºå®ƒåŒ…å«ä½ è®­ç»ƒæ—¶çš„æ‰€æœ‰é…ç½®ï¼ˆå¦‚ chat_templateï¼‰
print("æ­£åœ¨åŠ è½½ Tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(adapter_path)

# 2. åŠ è½½åŸºåº§æ¨¡å‹
# ä½¿ç”¨ device_map="auto"ï¼šä¼šè‡ªåŠ¨åˆ©ç”¨ GPUï¼Œæ¯”çº¯ CPU é€Ÿåº¦å¿«å‡ åå€
# low_cpu_mem_usage=Trueï¼šèƒ½æ˜¾è‘—é™ä½åŠ è½½æ—¶çš„å³°å€¼å†…å­˜å ç”¨ï¼Œé˜²æ­¢ "Killed"
print("æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹ (ä½¿ç”¨ GPU/CPU è‡ªåŠ¨åˆ†é…)...")
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto", 
    low_cpu_mem_usage=True,
    trust_remote_code=True
)

# 3. åŠ è½½é€‚é…å™¨
print("æ­£åœ¨åŠ è½½é€‚é…å™¨ (Adapter)...")
model = PeftModel.from_pretrained(
    base_model, 
    adapter_path,
    device_map="auto" # ä¿æŒå’ŒåŸºåº§æ¨¡å‹ä¸€è‡´çš„åˆ†é…é€»è¾‘
)

# 4. åˆå¹¶æƒé‡
# è¿™ä¸€æ­¥åœ¨ GPU ä¸Šè¿›è¡ŒçŸ©é˜µç›¸åŠ éå¸¸å¿«
print("æ­£åœ¨åˆå¹¶æƒé‡...")
merged_model = model.merge_and_unload()

# 5. ä¿å­˜åˆå¹¶åçš„å®Œæ•´æ¨¡å‹
# safe_serialization=True ä¼šä¿å­˜ä¸º .safetensors æ ¼å¼ï¼Œè¿™æ˜¯ç›®å‰æœ€æ¨èçš„æ ¼å¼
print(f"æ­£åœ¨ä¿å­˜å®Œæ•´æ¨¡å‹è‡³ {save_path}...")
merged_model.save_pretrained(save_path, safe_serialization=True)
tokenizer.save_pretrained(save_path)

print(f"\nğŸ‰ æˆåŠŸï¼æ¨¡å‹å·²æˆåŠŸåˆå¹¶å¹¶ä¿å­˜è‡³: {save_path}")